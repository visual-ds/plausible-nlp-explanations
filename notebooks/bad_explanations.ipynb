{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from experiments.datasets import HateXplain\n",
    "from experiments.explainers import LimeExplainer\n",
    "from experiments.metrics import alternative_auprc\n",
    "from experiments.models import DistilBertLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2023-03-14-22-37-07\"\n",
    "experiment = \"hatexplain-lime-distilbert-2\"\n",
    "\n",
    "here_path = Path().absolute()\n",
    "experiments_path = here_path.parent / \"data\" / \"experiments\"\n",
    "experiment_path = experiments_path / date / experiment\n",
    "results_path = experiment_path / \"results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "with open(results_path, 'r') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_results_with_weight(weight):\n",
    "    for result in results:\n",
    "        if result['weight'] == weight:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.821847995830308\n",
      "0.19537185696468617\n"
     ]
    }
   ],
   "source": [
    "greatest_alternative_auprc = 0\n",
    "preferred_result = None\n",
    "for result in results:\n",
    "    alternative_auprc_ = result['explainability']['alternative_auprc']\n",
    "    alternative_auprc_ = [x for x in alternative_auprc_ if x is not None and not np.isnan(x)]\n",
    "    alternative_auprc_ = np.mean(alternative_auprc_)\n",
    "    if  alternative_auprc_ > greatest_alternative_auprc:\n",
    "        greatest_alternative_auprc = alternative_auprc_\n",
    "        preferred_result = result\n",
    "print(greatest_alternative_auprc)\n",
    "print(preferred_result['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_result = select_results_with_weight(1)\n",
    "good_result = preferred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HateXplain(\n",
    "    negative_rationales=2,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"visual-ds/distilbert-base-uncased-hatexplain\"\n",
    "use_auth_token = True\n",
    "model_1 = DistilBertLogisticRegression(\n",
    "    model_name=model_name,\n",
    "    device=0,\n",
    "    batch_size=128,\n",
    "    random_state=46,\n",
    "    lr_tol=1e-4,\n",
    "    lr_C=1.0,\n",
    "    lr_max_iter=1e3,\n",
    "    n_jobs=1,\n",
    "    cross_val=True,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "model_1.fit(dataset.train_dataset, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = DistilBertLogisticRegression(\n",
    "    model_name=model_name,\n",
    "    device=0,\n",
    "    batch_size=128,\n",
    "    random_state=46,\n",
    "    lr_tol=1e-4,\n",
    "    lr_C=model_1.clf.C,\n",
    "    lr_max_iter=1e3,\n",
    "    n_jobs=1,\n",
    "    cross_val=False,\n",
    "    use_auth_token=use_auth_token,\n",
    ")\n",
    "model_2.fit(dataset.train_dataset, good_result['weight'], 1.0 - good_result['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_alternative_auprc = good_result['explainability']['alternative_auprc']\n",
    "# good_alternative_auprc = [x if x is not None and not np.isnan(x) else 0 for x in good_alternative_auprc]\n",
    "\n",
    "# bad_alternative_auprc = bad_result['explainability']['alternative_auprc']\n",
    "# bad_alternative_auprc = [x if x is not None and not np.isnan(x) else 0 for x in bad_alternative_auprc]\n",
    "\n",
    "# difference = np.array(good_alternative_auprc) - np.array(bad_alternative_auprc)\n",
    "# suggestions = np.argsort(difference)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_in_html(scores, sample):\n",
    "    max_abs_score = np.max(np.abs(scores))\n",
    "    scores = scores/max_abs_score\n",
    "\n",
    "    green = np.array([60, 179, 113])  # MediumSeaGreen\n",
    "    white = np.array([255, 255, 255])  # White\n",
    "    red = np.array([220, 20, 60])  # Crimson\n",
    "\n",
    "    colors = []\n",
    "    for score in scores:\n",
    "        if score >= 0:\n",
    "            color = score*green + (1 - score)*white\n",
    "            colors.append(color)\n",
    "        else:\n",
    "            score *= -1\n",
    "            color = score*red + (1 - score)*white\n",
    "            colors.append(color)\n",
    "    colors = [list(color) for color in colors]\n",
    "\n",
    "    html = \"\"\n",
    "    tokens = sample['tokens']\n",
    "    for token, color in zip(tokens, colors):\n",
    "        color = f\"rgb({color[0]},{color[1]},{color[2]})\"\n",
    "        span_style = f\"style=\\\"background-color:{color}\\\"\"\n",
    "        html += f\"<span {span_style}>{token}</span>\"\n",
    "        html += \" \"\n",
    "    html = html[:-1]\n",
    "\n",
    "    span_style = \"style=\\\"color:Black;background-color:White;\\\"\"\n",
    "    html = f\"<span {span_style}>{html}</span>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_explanation(scores):\n",
    "    scores = np.array(scores)\n",
    "    scores[scores < 0] = 0\n",
    "    max_value = np.max(scores)\n",
    "    min_value = np.min(scores)\n",
    "    length = len(scores)\n",
    "    steps = np.linspace(min_value, max_value, length)\n",
    "\n",
    "    indices = np.argsort(scores)\n",
    "    scores = np.array(scores)\n",
    "    rescaled_scores = np.zeros(length)\n",
    "    for index, step in zip(indices, steps):\n",
    "        rescaled_scores[index] = step\n",
    "\n",
    "    return rescaled_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.test_dataset[688]\n",
    "explainer = LimeExplainer(\n",
    "    num_samples=1000,\n",
    "    random_state=46,\n",
    "    split_token=dataset.split_token\n",
    ")\n",
    "bad_explanation = explainer.explain_sample(model_1, sample)\n",
    "bad_explanation = rescale_explanation(bad_explanation)\n",
    "print(bad_explanation)\n",
    "display(HTML(explanation_in_html(bad_explanation, sample)))\n",
    "good_explanation = explainer.explain_sample(model_2, sample)\n",
    "good_explanation = rescale_explanation(good_explanation)\n",
    "print(good_explanation)\n",
    "display(HTML(explanation_in_html(good_explanation, sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_auprc = good_result['explainability']['alternative_auprc']\n",
    "good_auprc = [x if x is not None and not np.isnan(x) else 0 for x in good_auprc]\n",
    "bad_auprc = bad_result['explainability']['alternative_auprc']\n",
    "bad_auprc = [x if x is not None and not np.isnan(x) else 0 for x in bad_auprc]\n",
    "auprc_diff = np.array(good_auprc) - np.array(bad_auprc)\n",
    "\n",
    "for i in np.argsort(auprc_diff)[::-1]:\n",
    "    sample = dataset.test_dataset[i]\n",
    "    if sample['label'] == \"normal\":\n",
    "        continue\n",
    "    explainer = LimeExplainer(\n",
    "        num_samples=1000,\n",
    "        random_state=46,\n",
    "        split_token=dataset.split_token,\n",
    "    )\n",
    "    bad_explanation = explainer.explain_sample(model_1, sample)\n",
    "    bad_auprc = alternative_auprc(bad_explanation, sample['rationales'], None, None, None, None)\n",
    "    good_explanation = explainer.explain_sample(model_2, sample)\n",
    "    good_auprc = alternative_auprc(good_explanation, sample['rationales'], None, None, None, None)\n",
    "    # if good_auprc - bad_auprc <= 0.5:\n",
    "    #     print(f\"Sample {i} has no good explanation: {good_auprc}, {bad_auprc}\")\n",
    "    #     continue\n",
    "\n",
    "    print(f\"Sample {i}, label {sample['label']}\")\n",
    "    display(HTML(dataset.sample_in_html(sample)))\n",
    "    n_rationales = sum(sample['rationales'])\n",
    "\n",
    "    predict_proba = model_1.predict_proba([sample])[0]\n",
    "    predicted_label = model_1.classes[np.argmax(predict_proba)]\n",
    "    print(f\"Bad explanation: {bad_auprc}; prediction: {predicted_label}\")\n",
    "    bad_explanation = rescale_explanation(bad_explanation)\n",
    "    display(HTML(explanation_in_html(bad_explanation, sample)))\n",
    "    argsort = np.argsort(bad_explanation)[::-1]\n",
    "    bad_explanation[argsort[:n_rationales]] = 1\n",
    "    bad_explanation[argsort[n_rationales:]] = 0\n",
    "    display(HTML(explanation_in_html(bad_explanation, sample)))\n",
    "    tokens = sample['tokens']\n",
    "    colorbox_tokens = [\n",
    "        r\"\\colorbox{white}{\" + token + \"}\" if explanation == 0 else\n",
    "        r\"\\colorbox{movie}{\" + token + \"}\" for token, explanation in\n",
    "        zip(tokens, bad_explanation)\n",
    "    ]\n",
    "    print(\"\".join(colorbox_tokens))\n",
    "\n",
    "    predict_proba = model_2.predict_proba([sample])[0]\n",
    "    predicted_label = model_2.classes[np.argmax(predict_proba)]\n",
    "    print(f\"Good explanation: {good_auprc}; prediction: {predicted_label}\")\n",
    "    good_explanation = rescale_explanation(good_explanation)\n",
    "    display(HTML(explanation_in_html(good_explanation, sample)))\n",
    "    argsort = np.argsort(good_explanation)[::-1]\n",
    "    good_explanation[argsort[:n_rationales]] = 1\n",
    "    good_explanation[argsort[n_rationales:]] = 0\n",
    "    display(HTML(explanation_in_html(good_explanation, sample)))\n",
    "    colorbox_tokens = [\n",
    "        r\"\\colorbox{white}{\" + token + \"}\" if explanation == 0 else\n",
    "        r\"\\colorbox{movie}{\" + token + \"}\" for token, explanation in\n",
    "        zip(tokens, good_explanation)\n",
    "    ]\n",
    "    print(\"\".join(colorbox_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c882aa1619578e48935015ad1cc542a3e08a6850e51962ef84a4f5af431fe8d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
